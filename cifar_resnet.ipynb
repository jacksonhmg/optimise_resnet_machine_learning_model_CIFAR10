{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jacksonhmg/optimise_resnet_machine_learning_model_CIFAR10/blob/main/cifar_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00783055",
      "metadata": {
        "id": "00783055"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db7bd171",
      "metadata": {
        "id": "db7bd171"
      },
      "outputs": [],
      "source": [
        "!pip install d2l==1.0.0a0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Steps to use full CIFAR-10 data instead of demo data given by d2l\n",
        "#1. Download the two files at https://drive.google.com/drive/folders/19pR1iTqmO9RZHEywxuDiKfmbp6KTQqUY\n",
        "#2. Go to your google drive, make a folder at the root dir named 'data'\n",
        "#3. Upload the two files you downloaded into the 'data' file\n",
        "#4. Uncomment the below code and run it\n",
        "#5. Go the files section in this session (left most nav bar)\n",
        "#6. Create a new folder in the immediate directory it gives you when you open the file navigator\n",
        "#7. Name that folder 'cifar-10'\n",
        "#8. From the newly created test and train folders you created from running the code below, drag them into the 'cifar-10' folder\n",
        "#9. Go to the kaggle link below and download the full cifar-10 files, unzip it. \n",
        "#10. Now that you have unzipped the 'trainLabels.csv' and the 'sampleSubmission.csv' files, drag them into this google colab session and into the 'cifar-10' folder you created\n",
        "#11. You should now have the 2 folders and the two csv's in the 'cifar-10' folder\n",
        "#12. Comment out the code below\n",
        "#13. Go to where \"demo = \" in one of the cells below, make sure you set it to false\n",
        "#14. Run the whole code\n",
        "\n",
        "#If you want to swap back between the demo data, all you have to do is change that variable \n",
        "#(unless your runtime session disconnects and then you'll have to redo all those last steps to get the full data back')\n",
        "\n",
        "#Code!!! :\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "#!unzip gdrive/MyDrive/data/train.zip\n",
        "#!unzip gdrive/MyDrive/data/test.zip"
      ],
      "metadata": {
        "id": "xdw9YPX6GoPq"
      },
      "id": "xdw9YPX6GoPq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9d6efc3c",
      "metadata": {
        "origin_pos": 0,
        "id": "9d6efc3c"
      },
      "source": [
        "# Image Classification (CIFAR-10) on Kaggle\n",
        ":label:`sec_kaggle_cifar10`\n",
        "\n",
        "So far, we have been using high-level APIs of deep learning frameworks to directly obtain image datasets in tensor format.\n",
        "However, custom image datasets\n",
        "often come in the form of image files.\n",
        "In this section, we will start from\n",
        "raw image files,\n",
        "and organize, read, then transform them\n",
        "into tensor format step by step.\n",
        "\n",
        "We experimented with the CIFAR-10 dataset in :numref:`sec_image_augmentation`,\n",
        "which is an important dataset in computer vision.\n",
        "In this section,\n",
        "we will apply the knowledge we learned\n",
        "in previous sections\n",
        "to practice the Kaggle competition of\n",
        "CIFAR-10 image classification.\n",
        "(**The web address of the competition is https://www.kaggle.com/c/cifar-10**)\n",
        "\n",
        ":numref:`fig_kaggle_cifar10` shows the information on the competition's webpage.\n",
        "In order to submit the results,\n",
        "you need to register a Kaggle account.\n",
        "\n",
        "![CIFAR-10 image classification competition webpage information. The competition dataset can be obtained by clicking the \"Data\" tab.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/kaggle-cifar10.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_kaggle_cifar10`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f141745",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:48.856450Z",
          "iopub.status.busy": "2023-02-10T06:06:48.856101Z",
          "iopub.status.idle": "2023-02-10T06:06:51.498876Z",
          "shell.execute_reply": "2023-02-10T06:06:51.497488Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "6f141745"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from d2l import torch as d2l\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b460d710",
      "metadata": {
        "origin_pos": 3,
        "id": "b460d710"
      },
      "source": [
        "## Obtaining and Organizing the Dataset\n",
        "\n",
        "The competition dataset is divided into\n",
        "a training set and a test set,\n",
        "which contain 50000 and 300000 images, respectively.\n",
        "In the test set,\n",
        "10000 images will be used for evaluation,\n",
        "while the remaining 290000 images will not\n",
        "be evaluated:\n",
        "they are included just\n",
        "to make it hard\n",
        "to cheat with\n",
        "*manually* labeled results of the test set.\n",
        "The images in this dataset\n",
        "are all png color (RGB channels) image files,\n",
        "whose height and width are both 32 pixels.\n",
        "The images cover a total of 10 categories, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, boats, and trucks.\n",
        "The upper-left corner of :numref:`fig_kaggle_cifar10` shows some images of airplanes, cars, and birds in the dataset.\n",
        "\n",
        "\n",
        "### Downloading the Dataset\n",
        "\n",
        "After logging in to Kaggle, we can click the \"Data\" tab on the CIFAR-10 image classification competition webpage shown in :numref:`fig_kaggle_cifar10` and download the dataset by clicking the \"Download All\" button.\n",
        "After unzipping the downloaded file in `../data`, and unzipping `train.7z` and `test.7z` inside it, you will find the entire dataset in the following paths:\n",
        "\n",
        "* `../data/cifar-10/train/[1-50000].png`\n",
        "* `../data/cifar-10/test/[1-300000].png`\n",
        "* `../data/cifar-10/trainLabels.csv`\n",
        "* `../data/cifar-10/sampleSubmission.csv`\n",
        "\n",
        "where the `train` and `test` directories contain the training and testing images, respectively, `trainLabels.csv` provides labels for the training images, and `sample_submission.csv` is a sample submission file.\n",
        "\n",
        "To make it easier to get started, [**we provide a small-scale sample of the dataset that\n",
        "contains the first 1000 training images and 5 random testing images.**]\n",
        "To use the full dataset of the Kaggle competition, you need to set the following `demo` variable to `False`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08102f24",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:51.503640Z",
          "iopub.status.busy": "2023-02-10T06:06:51.503147Z",
          "iopub.status.idle": "2023-02-10T06:06:52.111321Z",
          "shell.execute_reply": "2023-02-10T06:06:52.110167Z"
        },
        "origin_pos": 4,
        "tab": [
          "pytorch"
        ],
        "id": "08102f24"
      },
      "outputs": [],
      "source": [
        "d2l.DATA_HUB['cifar10_tiny'] = (d2l.DATA_URL + 'kaggle_cifar10_tiny.zip',\n",
        "                                '2068874e4b9a9f0fb07ebe0ad2b29754449ccacd')\n",
        "# If you use the full dataset downloaded for the Kaggle competition, set\n",
        "# `demo` to False\n",
        "demo = True\n",
        "if demo:\n",
        "    data_dir = d2l.download_extract('cifar10_tiny')\n",
        "else:\n",
        "    data_dir = 'cifar-10/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb9c1ec",
      "metadata": {
        "origin_pos": 5,
        "id": "beb9c1ec"
      },
      "source": [
        "### [**Organizing the Dataset**]\n",
        "\n",
        "We need to organize datasets to facilitate model training and testing.\n",
        "Let's first read the labels from the csv file.\n",
        "The following function returns a dictionary that maps\n",
        "the non-extension part of the filename to its label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd524ce2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.118145Z",
          "iopub.status.busy": "2023-02-10T06:06:52.115535Z",
          "iopub.status.idle": "2023-02-10T06:06:52.125265Z",
          "shell.execute_reply": "2023-02-10T06:06:52.124462Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "fd524ce2",
        "outputId": "7b788e48-a1bd-4868-9662-4bb388bd6347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training examples: 50000\n",
            "# classes: 10\n"
          ]
        }
      ],
      "source": [
        "def read_csv_labels(fname):\n",
        "    \"\"\"Read `fname` to return a filename to label dictionary.\"\"\"\n",
        "    with open(fname, 'r') as f:\n",
        "        # Skip the file header line (column name)\n",
        "        lines = f.readlines()[1:]\n",
        "    tokens = [l.rstrip().split(',') for l in lines]\n",
        "    return dict(((name, label) for name, label in tokens))\n",
        "\n",
        "labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
        "print('# training examples:', len(labels))\n",
        "print('# classes:', len(set(labels.values())))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c9b896",
      "metadata": {
        "origin_pos": 7,
        "id": "59c9b896"
      },
      "source": [
        "Next, we define the `reorg_train_valid` function to [**split the validation set out of the original training set.**]\n",
        "The argument `valid_ratio` in this function is the ratio of the number of examples in the validation set to the number of examples in the original training set.\n",
        "More concretely,\n",
        "let $n$ be the number of images of the class with the least examples, and $r$ be the ratio.\n",
        "The validation set will split out\n",
        "$\\max(\\lfloor nr\\rfloor,1)$ images for each class.\n",
        "Let's use `valid_ratio=0.1` as an example. Since the original training set has 50000 images,\n",
        "there will be 45000 images used for training in the path `train_valid_test/train`,\n",
        "while the other 5000 images will be split out\n",
        "as validation set in the path `train_valid_test/valid`. After organizing the dataset, images of the same class will be placed under the same folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33525c58",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.129148Z",
          "iopub.status.busy": "2023-02-10T06:06:52.128820Z",
          "iopub.status.idle": "2023-02-10T06:06:52.139103Z",
          "shell.execute_reply": "2023-02-10T06:06:52.138256Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "33525c58"
      },
      "outputs": [],
      "source": [
        "def copyfile(filename, target_dir):\n",
        "    \"\"\"Copy a file into a target directory.\"\"\"\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    shutil.copy(filename, target_dir)\n",
        "\n",
        "def reorg_train_valid(data_dir, labels, valid_ratio):\n",
        "    \"\"\"Split the validation set out of the original training set.\"\"\"\n",
        "    # The number of examples of the class that has the fewest examples in the\n",
        "    # training dataset\n",
        "    n = collections.Counter(labels.values()).most_common()[-1][1]\n",
        "    # The number of examples per class for the validation set\n",
        "    n_valid_per_label = max(1, math.floor(n * valid_ratio))\n",
        "    label_count = {}\n",
        "    for train_file in os.listdir(os.path.join(data_dir, 'train')):\n",
        "        label = labels[train_file.split('.')[0]]\n",
        "        fname = os.path.join(data_dir, 'train', train_file)\n",
        "        copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
        "                                     'train_valid', label))\n",
        "        if label not in label_count or label_count[label] < n_valid_per_label:\n",
        "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
        "                                         'valid', label))\n",
        "            label_count[label] = label_count.get(label, 0) + 1\n",
        "        else:\n",
        "            copyfile(fname, os.path.join(data_dir, 'train_valid_test',\n",
        "                                         'train', label))\n",
        "    return n_valid_per_label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffa2a93a",
      "metadata": {
        "origin_pos": 9,
        "id": "ffa2a93a"
      },
      "source": [
        "The `reorg_test` function below [**organizes the testing set for data loading during prediction.**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef81b1c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.142635Z",
          "iopub.status.busy": "2023-02-10T06:06:52.142206Z",
          "iopub.status.idle": "2023-02-10T06:06:52.147893Z",
          "shell.execute_reply": "2023-02-10T06:06:52.147074Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "4ef81b1c"
      },
      "outputs": [],
      "source": [
        "def reorg_test(data_dir):\n",
        "    \"\"\"Organize the testing set for data loading during prediction.\"\"\"\n",
        "    for test_file in os.listdir(os.path.join(data_dir, 'test')):\n",
        "        copyfile(os.path.join(data_dir, 'test', test_file),\n",
        "                 os.path.join(data_dir, 'train_valid_test', 'test',\n",
        "                              'unknown'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76086dd1",
      "metadata": {
        "origin_pos": 11,
        "id": "76086dd1"
      },
      "source": [
        "Finally, we use a function to [**invoke**]\n",
        "the `read_csv_labels`, `reorg_train_valid`, and `reorg_test` (**functions defined above.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d730a308",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.151711Z",
          "iopub.status.busy": "2023-02-10T06:06:52.151149Z",
          "iopub.status.idle": "2023-02-10T06:06:52.157104Z",
          "shell.execute_reply": "2023-02-10T06:06:52.156405Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "d730a308"
      },
      "outputs": [],
      "source": [
        "def reorg_cifar10_data(data_dir, valid_ratio):\n",
        "    labels = read_csv_labels(os.path.join(data_dir, 'trainLabels.csv'))\n",
        "    reorg_train_valid(data_dir, labels, valid_ratio)\n",
        "    reorg_test(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c8d640",
      "metadata": {
        "origin_pos": 13,
        "id": "07c8d640"
      },
      "source": [
        "Here we only set the batch size to 32 for the small-scale sample of the dataset.\n",
        "When training and testing\n",
        "the complete dataset of the Kaggle competition,\n",
        "`batch_size` should be set to a larger integer, such as 128.\n",
        "We split out 10% of the training examples as the validation set for tuning hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "650f7dad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.160941Z",
          "iopub.status.busy": "2023-02-10T06:06:52.160609Z",
          "iopub.status.idle": "2023-02-10T06:06:52.539120Z",
          "shell.execute_reply": "2023-02-10T06:06:52.537928Z"
        },
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "650f7dad"
      },
      "outputs": [],
      "source": [
        "batch_size = 32 if demo else 128\n",
        "#batch_size = 128\n",
        "valid_ratio = 0.1\n",
        "reorg_cifar10_data(data_dir, valid_ratio)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121ff4df",
      "metadata": {
        "origin_pos": 15,
        "id": "121ff4df"
      },
      "source": [
        "## [**Image Augmentation**]\n",
        "\n",
        "We use image augmentation to address overfitting.\n",
        "For example, images can be flipped horizontally at random during training.\n",
        "We can also perform standardization for the three RGB channels of color images. Below lists some of these operations that you can tweak.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd50bdb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.543155Z",
          "iopub.status.busy": "2023-02-10T06:06:52.542823Z",
          "iopub.status.idle": "2023-02-10T06:06:52.549883Z",
          "shell.execute_reply": "2023-02-10T06:06:52.549144Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "7cd50bdb"
      },
      "outputs": [],
      "source": [
        "transform_train = torchvision.transforms.Compose([\n",
        "    # Scale the image up to a square of 40 pixels in both height and width\n",
        "    torchvision.transforms.Resize(40),\n",
        "    # Randomly crop a square image of 40 pixels in both height and width to\n",
        "    # produce a small square of 0.64 to 1 times the area of the original\n",
        "    # image, and then scale it to a square of 32 pixels in both height and\n",
        "    # width\n",
        "    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),\n",
        "                                                   ratio=(1.0, 1.0)),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    # Standardize each channel of the image\n",
        "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
        "                                     [0.2023, 0.1994, 0.2010])])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0566cb",
      "metadata": {
        "origin_pos": 18,
        "id": "1c0566cb"
      },
      "source": [
        "During testing,\n",
        "we only perform standardization on images\n",
        "so as to\n",
        "remove randomness in the evaluation results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911d60f9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.552984Z",
          "iopub.status.busy": "2023-02-10T06:06:52.552538Z",
          "iopub.status.idle": "2023-02-10T06:06:52.557982Z",
          "shell.execute_reply": "2023-02-10T06:06:52.557001Z"
        },
        "origin_pos": 20,
        "tab": [
          "pytorch"
        ],
        "id": "911d60f9"
      },
      "outputs": [],
      "source": [
        "transform_test = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
        "                                     [0.2023, 0.1994, 0.2010])])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b89abc",
      "metadata": {
        "origin_pos": 21,
        "id": "f0b89abc"
      },
      "source": [
        "## Reading the Dataset\n",
        "\n",
        "Next, we [**read the organized dataset consisting of raw image files**]. Each example includes an image and a label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc39f97",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.561152Z",
          "iopub.status.busy": "2023-02-10T06:06:52.560736Z",
          "iopub.status.idle": "2023-02-10T06:06:52.576945Z",
          "shell.execute_reply": "2023-02-10T06:06:52.576017Z"
        },
        "origin_pos": 23,
        "tab": [
          "pytorch"
        ],
        "id": "0bc39f97"
      },
      "outputs": [],
      "source": [
        "train_ds, train_valid_ds = [torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, 'train_valid_test', folder),\n",
        "    transform=transform_train) for folder in ['train', 'train_valid']]\n",
        "\n",
        "valid_ds, test_ds = [torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, 'train_valid_test', folder),\n",
        "    transform=transform_test) for folder in ['valid', 'test']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6e343f",
      "metadata": {
        "origin_pos": 24,
        "id": "6b6e343f"
      },
      "source": [
        "During training,\n",
        "we need to [**specify all the image augmentation operations defined above**].\n",
        "When the validation set\n",
        "is used for model evaluation during hyperparameter tuning,\n",
        "no randomness from image augmentation should be introduced.\n",
        "Before final prediction,\n",
        "we train the model on the combined training set and validation set to make full use of all the labeled data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "802d7da1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.580713Z",
          "iopub.status.busy": "2023-02-10T06:06:52.580244Z",
          "iopub.status.idle": "2023-02-10T06:06:52.587571Z",
          "shell.execute_reply": "2023-02-10T06:06:52.586596Z"
        },
        "origin_pos": 26,
        "tab": [
          "pytorch"
        ],
        "id": "802d7da1"
      },
      "outputs": [],
      "source": [
        "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
        "    dataset, batch_size, shuffle=True, drop_last=True)\n",
        "    for dataset in (train_ds, train_valid_ds)]\n",
        "\n",
        "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n",
        "                                         drop_last=True)\n",
        "\n",
        "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n",
        "                                        drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d6cea8",
      "metadata": {
        "origin_pos": 27,
        "id": "b2d6cea8"
      },
      "source": [
        "## Defining the [**Model**]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59178a30",
      "metadata": {
        "origin_pos": 33,
        "tab": [
          "pytorch"
        ],
        "id": "59178a30"
      },
      "source": [
        "We define the ResNet-18 model described in\n",
        ":numref:`sec_resnet`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    \"\"\"The Residual block of ResNet models.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
        "                                   stride=strides)\n",
        "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
        "                                       stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.LazyBatchNorm2d()\n",
        "        self.bn2 = nn.LazyBatchNorm2d()\n",
        "\n",
        "    def forward(self, X):\n",
        "        #Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        #Y = self.bn2(self.conv2(Y))\n",
        "        Y = self.bn1(self.conv1(X))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)\n",
        "\n",
        "\n",
        "blk = Residual(3)\n",
        "X = torch.randn(4, 3, 6, 6)\n",
        "blk(X).shape\n",
        "\n",
        "blk = Residual(6, use_1x1conv=True, strides=2)\n",
        "blk(X).shape\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import collections\n",
        "\n",
        "class ProgressBoard(d2l.HyperParameters):\n",
        "    \"\"\"The board that plots data points in animation.\n",
        "\n",
        "    Defined in :numref:`sec_oo-design`\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 ls=['-', '--', '-.', ':'], colors=['C0', 'C1', 'C2', 'C3'],\n",
        "                 fig=None, axes=None, figsize=(3.5, 2.5), display=True):\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def draw(self, x, y, label, every_n=1):\n",
        "        raise NotImplemented\n",
        "\n",
        "    def draw(self, x, y, label, every_n=1):\n",
        "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
        "        Point = collections.namedtuple('Point', ['x', 'y'])\n",
        "        if not hasattr(self, 'raw_points'):\n",
        "            self.raw_points = collections.OrderedDict()\n",
        "            self.data = collections.OrderedDict()\n",
        "        if label not in self.raw_points:\n",
        "            self.raw_points[label] = []\n",
        "            self.data[label] = []\n",
        "        points = self.raw_points[label]\n",
        "        line = self.data[label]\n",
        "        points.append(Point(x, y))\n",
        "        #if label == \"val_acc\":\n",
        "        #  print(\"val acc = \", y);\n",
        "        if len(points) != every_n:\n",
        "            return\n",
        "        mean = lambda x: sum(x) / len(x)\n",
        "        line.append(Point(mean([p.x for p in points]),\n",
        "                          mean([p.y for p in points])))\n",
        "        #print(mean([p.y for p in points]))\n",
        "        points.clear()\n",
        "        if not self.display:\n",
        "            return\n",
        "        d2l.use_svg_display()\n",
        "        if self.fig is None:\n",
        "            self.fig = d2l.plt.figure(figsize=self.figsize)\n",
        "        plt_lines, labels = [], []\n",
        "        for (k, v), ls, color in zip(self.data.items(), self.ls, self.colors):\n",
        "            plt_lines.append(d2l.plt.plot([p.x for p in v], [p.y for p in v],\n",
        "                                          linestyle=ls, color=color)[0])\n",
        "            labels.append(k)\n",
        "        axes = self.axes if self.axes else d2l.plt.gca()\n",
        "        if self.xlim: axes.set_xlim(self.xlim)\n",
        "        if self.ylim: axes.set_ylim(self.ylim)\n",
        "        if not self.xlabel: self.xlabel = self.x\n",
        "        axes.set_xlabel(self.xlabel)\n",
        "        axes.set_ylabel(self.ylabel)\n",
        "        axes.set_xscale(self.xscale)\n",
        "        axes.set_yscale(self.yscale)\n",
        "        axes.legend(plt_lines, labels)\n",
        "        #display.display(self.fig)\n",
        "        #display.clear_output(wait=True)\n",
        "\n",
        "        \n",
        "        ##if label == 'val_acc':\n",
        "        ##  print(\"val acc = \", y)\n",
        "        print(f\"Epoch: {x}, {label}: {y}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Module(d2l.nn_Module, d2l.HyperParameters):\n",
        "    \"\"\"The base class of models.\n",
        "\n",
        "    Defined in :numref:`sec_oo-design`\"\"\"\n",
        "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.board = ProgressBoard()\n",
        "\n",
        "    def loss(self, y_hat, y):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, X):\n",
        "        assert hasattr(self, 'net'), 'Neural network is defined'\n",
        "        return self.net(X)\n",
        "\n",
        "    def plot(self, key, value, train):\n",
        "        \"\"\"Plot a point in animation.\"\"\"\n",
        "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
        "        self.board.xlabel = 'epoch'\n",
        "        if train:\n",
        "            x = self.trainer.train_batch_idx / \\\n",
        "                self.trainer.num_train_batches\n",
        "            n = self.trainer.num_train_batches / \\\n",
        "                self.plot_train_per_epoch\n",
        "        else:\n",
        "            x = self.trainer.epoch + 1\n",
        "            n = self.trainer.num_val_batches / \\\n",
        "                self.plot_valid_per_epoch\n",
        "        self.board.draw(x, d2l.numpy(d2l.to(value, d2l.cpu())),\n",
        "                        ('train_' if train else 'val_') + key,\n",
        "                        every_n=int(n))\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        self.plot('loss', l, train=True)\n",
        "        return l\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
        "        self.plot('loss', l, train=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Defined in :numref:`sec_classification`\"\"\"\n",
        "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def apply_init(self, inputs, init=None):\n",
        "        \"\"\"Defined in :numref:`sec_lazy_init`\"\"\"\n",
        "        self.forward(*inputs)\n",
        "        if init is not None:\n",
        "            self.net.apply(init)\n",
        "\n",
        "\n",
        "class Classifier(Module):\n",
        "    \"\"\"The base class of classification models.\n",
        "\n",
        "    Defined in :numref:`sec_classification`\"\"\"\n",
        "    def validation_step(self, batch):\n",
        "        Y_hat = self(*batch[:-1])\n",
        "        x = self.accuracy(Y_hat, batch[-1]);\n",
        "        ##print(x)\n",
        "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
        "        self.plot('acc', x, train=False)\n",
        "\n",
        "    def accuracy(self, Y_hat, Y, averaged=True):\n",
        "        \"\"\"Compute the number of correct predictions.\n",
        "    \n",
        "        Defined in :numref:`sec_classification`\"\"\"\n",
        "        Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
        "        preds = d2l.astype(d2l.argmax(Y_hat, axis=1), Y.dtype)\n",
        "        compare = d2l.astype(preds == d2l.reshape(Y, -1), d2l.float32)\n",
        "        return d2l.reduce_mean(compare) if averaged else compare\n",
        "\n",
        "    def loss(self, Y_hat, Y, averaged=True):\n",
        "        \"\"\"Defined in :numref:`sec_softmax_concise`\"\"\"\n",
        "        Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
        "        Y = d2l.reshape(Y, (-1,))\n",
        "        return F.cross_entropy(\n",
        "            Y_hat, Y, reduction='mean' if averaged else 'none')\n",
        "\n",
        "    def layer_summary(self, X_shape):\n",
        "        \"\"\"Defined in :numref:`sec_lenet`\"\"\"\n",
        "        X = d2l.randn(*X_shape)\n",
        "        for layer in self.net:\n",
        "            X = layer(X)\n",
        "            print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
        "\n",
        "\n",
        "class ResNet(Classifier):\n",
        "    def b1(self):\n",
        "        return nn.Sequential(\n",
        "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LazyBatchNorm2d(), \n",
        "            ##nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "        \n",
        "\n",
        "@d2l.add_to_class(ResNet)\n",
        "def block(self, num_residuals, num_channels, first_block=False):\n",
        "    blk = []\n",
        "    for i in range(num_residuals):\n",
        "        if i == 0 and not first_block:\n",
        "            blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "        else:\n",
        "            blk.append(Residual(num_channels))\n",
        "    return nn.Sequential(*blk)\n",
        "\n",
        "\n",
        "@d2l.add_to_class(ResNet)\n",
        "def __init__(self, arch, lr=0.1, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.save_hyperparameters()\n",
        "    self.net = nn.Sequential(self.b1())\n",
        "    for i, b in enumerate(arch):\n",
        "        self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
        "    self.net.add_module('last', nn.Sequential(\n",
        "        nn.AdaptiveMaxPool2d((1, 1)), nn.Flatten(),\n",
        "        nn.LazyLinear(num_classes)))\n",
        "    self.net.apply(d2l.init_cnn)\n",
        "\n",
        "\n",
        "class ResNet18(ResNet):\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
        "                       lr, num_classes)\n",
        "        \n",
        "ResNet18().layer_summary((1, 1, 96, 96))\n",
        "\n",
        "model = ResNet18(lr=0.08)\n"
      ],
      "metadata": {
        "id": "f7yhOd74NdI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fbcecb-3900-416d-d14f-a85921d84beb"
      },
      "id": "f7yhOd74NdI4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
            "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
            "Sequential output shape:\t torch.Size([1, 128, 12, 12])\n",
            "Sequential output shape:\t torch.Size([1, 256, 6, 6])\n",
            "Sequential output shape:\t torch.Size([1, 512, 3, 3])\n",
            "Sequential output shape:\t torch.Size([1, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f282084c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.591871Z",
          "iopub.status.busy": "2023-02-10T06:06:52.591405Z",
          "iopub.status.idle": "2023-02-10T06:06:52.598638Z",
          "shell.execute_reply": "2023-02-10T06:06:52.597557Z"
        },
        "origin_pos": 35,
        "tab": [
          "pytorch"
        ],
        "id": "f282084c"
      },
      "outputs": [],
      "source": [
        "def get_net():\n",
        "    net = model\n",
        "    return net\n",
        "\n",
        "loss = nn.CrossEntropyLoss(reduction=\"none\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c602f6b9",
      "metadata": {
        "origin_pos": 36,
        "id": "c602f6b9"
      },
      "source": [
        "## Defining the [**Training Function**]\n",
        "\n",
        "We will select models and tune hyperparameters according to the model's performance on the validation set.\n",
        "In the following, we define the model training function `train`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f930b984",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.602234Z",
          "iopub.status.busy": "2023-02-10T06:06:52.601708Z",
          "iopub.status.idle": "2023-02-10T06:06:52.616360Z",
          "shell.execute_reply": "2023-02-10T06:06:52.615496Z"
        },
        "origin_pos": 38,
        "tab": [
          "pytorch"
        ],
        "id": "f930b984"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
        "          lr_decay):\n",
        "    trainer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
        "                              weight_decay=wd)\n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_period, lr_decay)\n",
        "\n",
        "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(trainer, num_epochs, 2e-10)\n",
        "\n",
        "    num_batches, timer = len(train_iter), d2l.Timer()\n",
        "    legend = ['train loss', 'train acc']\n",
        "    if valid_iter is not None:\n",
        "        legend.append('valid acc')\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                            legend=legend)\n",
        "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        metric = d2l.Accumulator(3)\n",
        "        for i, (features, labels) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            l, acc = d2l.train_batch_ch13(net, features, labels,\n",
        "                                          loss, trainer, devices)\n",
        "            metric.add(l, acc, labels.shape[0])\n",
        "            timer.stop()\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                             (metric[0] / metric[2], metric[1] / metric[2],\n",
        "                              None))\n",
        "        if valid_iter is not None:\n",
        "            valid_acc = d2l.evaluate_accuracy_gpu(net, valid_iter)\n",
        "            animator.add(epoch + 1, (None, None, valid_acc))\n",
        "        scheduler.step()\n",
        "    measures = (f'train loss {metric[0] / metric[2]:.3f}, '\n",
        "                f'train acc {metric[1] / metric[2]:.3f}')\n",
        "    if valid_iter is not None:\n",
        "        measures += f', valid acc {valid_acc:.3f}'\n",
        "    print(measures + f'\\n{metric[2] * num_epochs / timer.sum():.1f}'\n",
        "          f' examples/sec on {str(devices)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca6cea0b",
      "metadata": {
        "origin_pos": 39,
        "id": "ca6cea0b"
      },
      "source": [
        "## [**Training and Validating the Model**]\n",
        "\n",
        "Now, we can train and validate the model.\n",
        "All the following hyperparameters can be tuned.\n",
        "For example, we can increase the number of epochs.\n",
        "When `lr_period` and `lr_decay` are set to 4 and 0.9, respectively, the learning rate of the optimization algorithm will be multiplied by 0.9 after every 4 epochs. Just for ease of demonstration,\n",
        "we only train 20 epochs here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61de1641",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-10T06:06:52.619451Z",
          "iopub.status.busy": "2023-02-10T06:06:52.619082Z",
          "iopub.status.idle": "2023-02-10T06:08:08.572894Z",
          "shell.execute_reply": "2023-02-10T06:08:08.571613Z"
        },
        "origin_pos": 41,
        "tab": [
          "pytorch"
        ],
        "id": "61de1641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "10f08069-52cd-4aed-fde3-90532a9eb300"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 0.321, train acc 0.890, valid acc 0.767\n",
            "4606.7 examples/sec on [device(type='cuda', index=0)]\n",
            "1172.33 seconds to execute 20 epochs\n",
            "Therefore 58.62 seconds per epoch\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 350x250 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"238.965625pt\" height=\"185.941319pt\" viewBox=\"0 0 238.965625 185.941319\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-05-28T04:29:24.747114</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 185.941319 \nL 238.965625 185.941319 \nL 238.965625 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 148.385069 \nL 225.403125 148.385069 \nL 225.403125 9.785069 \nL 30.103125 9.785069 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path d=\"M 71.218914 148.385069 \nL 71.218914 9.785069 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path id=\"m65a9fa3d5a\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m65a9fa3d5a\" x=\"71.218914\" y=\"148.385069\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 5 -->\n      <g transform=\"translate(68.037664 162.983506) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path d=\"M 122.613651 148.385069 \nL 122.613651 9.785069 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m65a9fa3d5a\" x=\"122.613651\" y=\"148.385069\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(116.251151 162.983506) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path d=\"M 174.008388 148.385069 \nL 174.008388 9.785069 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m65a9fa3d5a\" x=\"174.008388\" y=\"148.385069\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 15 -->\n      <g transform=\"translate(167.645888 162.983506) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path d=\"M 225.403125 148.385069 \nL 225.403125 9.785069 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m65a9fa3d5a\" x=\"225.403125\" y=\"148.385069\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 20 -->\n      <g transform=\"translate(219.040625 162.983506) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- epoch -->\n     <g transform=\"translate(112.525 176.661631) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path d=\"M 30.103125 128.035602 \nL 225.403125 128.035602 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path id=\"m70788d5187\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m70788d5187\" x=\"30.103125\" y=\"128.035602\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 131.834821) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path d=\"M 30.103125 89.023475 \nL 225.403125 89.023475 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m70788d5187\" x=\"30.103125\" y=\"89.023475\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 92.822693) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path d=\"M 30.103125 50.011347 \nL 225.403125 50.011347 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#m70788d5187\" x=\"30.103125\" y=\"50.011347\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1.5 -->\n      <g transform=\"translate(7.2 53.810565) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_15\">\n      <path d=\"M 30.103125 10.999219 \nL 225.403125 10.999219 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use xlink:href=\"#m70788d5187\" x=\"30.103125\" y=\"10.999219\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 14.798437) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 21.87411 16.085069 \nL 23.924043 28.294246 \nL 25.973975 34.845711 \nL 28.023908 40.531838 \nL 30.07384 44.446655 \nL 30.103125 44.499998 \nL 32.153058 66.331897 \nL 34.20299 67.931375 \nL 36.252923 69.682416 \nL 38.302855 71.079514 \nL 40.352788 72.379957 \nL 40.382072 72.421506 \nL 42.432005 83.80696 \nL 44.481937 84.002476 \nL 46.53187 84.850974 \nL 48.581802 85.24449 \nL 50.631735 86.205144 \nL 50.66102 86.21998 \nL 52.710952 92.821851 \nL 54.760885 94.267798 \nL 56.810817 95.178234 \nL 58.86075 95.298572 \nL 60.910682 95.602718 \nL 60.939967 95.617528 \nL 62.9899 105.227539 \nL 65.039832 106.077022 \nL 67.089765 106.584072 \nL 69.139697 106.918965 \nL 71.18963 106.950771 \nL 71.218914 106.949885 \nL 73.268847 112.303131 \nL 75.31878 112.27253 \nL 77.368712 112.090808 \nL 79.418645 111.75589 \nL 81.468577 111.72889 \nL 81.497862 111.762764 \nL 83.547794 117.625226 \nL 85.597727 115.308059 \nL 87.647659 114.902676 \nL 89.697592 114.703821 \nL 91.747524 114.546362 \nL 91.776809 114.550064 \nL 93.826742 117.534846 \nL 95.876674 117.424394 \nL 97.926607 117.48563 \nL 99.976539 117.184969 \nL 102.026472 117.102608 \nL 102.055757 117.09422 \nL 104.105689 123.340327 \nL 106.155622 122.992979 \nL 108.205554 123.105873 \nL 110.255487 123.133462 \nL 112.305419 123.348817 \nL 112.334704 123.342147 \nL 114.384636 124.865076 \nL 116.434569 125.796332 \nL 118.484502 125.514572 \nL 120.534434 125.793975 \nL 122.584367 125.84704 \nL 122.613651 125.8377 \nL 124.663584 128.247441 \nL 126.713516 128.525631 \nL 128.763449 128.300612 \nL 130.813381 127.889314 \nL 132.863314 127.570399 \nL 132.892599 127.570292 \nL 134.942531 129.718687 \nL 136.992464 129.367502 \nL 139.042396 128.935959 \nL 141.092329 128.556858 \nL 143.142261 128.704841 \nL 143.171546 128.719848 \nL 145.221479 134.307302 \nL 147.271411 133.05728 \nL 149.321344 133.181165 \nL 151.371276 133.009365 \nL 153.421209 133.23145 \nL 153.450493 133.227679 \nL 155.500426 135.612533 \nL 157.550358 135.093326 \nL 159.600291 135.036056 \nL 161.650224 134.999488 \nL 163.700156 134.732088 \nL 163.729441 134.74351 \nL 165.779373 136.283221 \nL 167.829306 135.955976 \nL 169.879238 135.880786 \nL 171.929171 135.944924 \nL 173.979103 135.89064 \nL 174.008388 135.910118 \nL 176.058321 137.613328 \nL 178.108253 137.564536 \nL 180.158186 137.280761 \nL 182.208118 137.233963 \nL 184.258051 137.121407 \nL 184.287336 137.121328 \nL 186.337268 139.977908 \nL 188.387201 139.469905 \nL 190.437133 139.925066 \nL 192.487066 140.041974 \nL 194.536998 139.71271 \nL 194.566283 139.702459 \nL 196.616215 140.610605 \nL 198.666148 140.703934 \nL 200.71608 140.830164 \nL 202.766013 140.994116 \nL 204.815946 140.733561 \nL 204.84523 140.722724 \nL 206.895163 141.707456 \nL 208.945095 141.589993 \nL 210.995028 141.443274 \nL 213.04496 141.465305 \nL 215.094893 141.567606 \nL 215.124178 141.584157 \nL 217.17411 141.451471 \nL 219.224043 142.085069 \nL 221.273975 141.773367 \nL 223.323908 142.02847 \nL 225.37384 141.989127 \nL 225.403125 141.995939 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 21.87411 140.810333 \nL 23.924043 137.658013 \nL 25.973975 135.82932 \nL 28.023908 133.920077 \nL 30.07384 132.52548 \nL 30.103125 132.512689 \nL 32.153058 124.865867 \nL 34.20299 124.265011 \nL 36.252923 123.53063 \nL 38.302855 123.156909 \nL 40.352788 122.549522 \nL 40.382072 122.537365 \nL 42.432005 118.212906 \nL 44.481937 118.056161 \nL 46.53187 117.733962 \nL 48.581802 117.612049 \nL 50.631735 117.411764 \nL 50.66102 117.410772 \nL 52.710952 115.644027 \nL 54.760885 114.943028 \nL 56.810817 114.433606 \nL 58.86075 114.322578 \nL 60.910682 114.203713 \nL 60.939967 114.194494 \nL 62.9899 110.59335 \nL 65.039832 110.471437 \nL 67.089765 110.276957 \nL 69.139697 110.153592 \nL 71.18963 110.147497 \nL 71.218914 110.144624 \nL 73.268847 108.47729 \nL 75.31878 108.307483 \nL 77.368712 108.346669 \nL 79.418645 108.440281 \nL 81.468577 108.485998 \nL 81.497862 108.468756 \nL 83.547794 106.378647 \nL 85.597727 107.279931 \nL 87.647659 107.467155 \nL 89.697592 107.475863 \nL 91.747524 107.519403 \nL 91.776809 107.511861 \nL 93.826742 106.58764 \nL 95.876674 106.461373 \nL 97.926607 106.372841 \nL 99.976539 106.500559 \nL 102.026472 106.540617 \nL 102.055757 106.544546 \nL 104.105689 104.34096 \nL 106.155622 104.580431 \nL 108.205554 104.515121 \nL 110.255487 104.567369 \nL 112.305419 104.473322 \nL 112.334704 104.476195 \nL 114.384636 104.036177 \nL 116.434569 103.653022 \nL 118.484502 103.766227 \nL 120.534434 103.565942 \nL 122.584367 103.581616 \nL 122.613651 103.593976 \nL 124.663584 102.729968 \nL 126.713516 102.642887 \nL 128.763449 102.686427 \nL 130.813381 102.825756 \nL 132.863314 102.98076 \nL 132.892599 102.982675 \nL 134.942531 102.094279 \nL 136.992464 102.307627 \nL 139.042396 102.332299 \nL 141.092329 102.527505 \nL 143.142261 102.4844 \nL 143.171546 102.47731 \nL 145.221479 100.805485 \nL 147.271411 101.149454 \nL 149.321344 101.10156 \nL 151.371276 101.097206 \nL 153.421209 100.934365 \nL 153.450493 100.938637 \nL 155.500426 100.047884 \nL 157.550358 100.239461 \nL 159.600291 100.193018 \nL 161.650224 100.217691 \nL 163.700156 100.347441 \nL 163.729441 100.339492 \nL 165.779373 99.690853 \nL 167.829306 99.908555 \nL 169.879238 99.949192 \nL 171.929171 99.982573 \nL 173.979103 99.976478 \nL 174.008388 99.967849 \nL 176.058321 99.264158 \nL 178.108253 99.373009 \nL 180.158186 99.458638 \nL 182.208118 99.416549 \nL 184.258051 99.426128 \nL 184.287336 99.420804 \nL 186.337268 98.227898 \nL 188.387201 98.611053 \nL 190.437133 98.422379 \nL 192.487066 98.38029 \nL 194.536998 98.513523 \nL 194.566283 98.521219 \nL 196.616215 98.227898 \nL 198.666148 98.210482 \nL 200.71608 98.140818 \nL 202.766013 98.031967 \nL 204.815946 98.213966 \nL 204.84523 98.213832 \nL 206.895163 98.071153 \nL 208.945095 97.997135 \nL 210.995028 97.931824 \nL 213.04496 97.960125 \nL 215.094893 97.91615 \nL 215.124178 97.911654 \nL 217.17411 97.696707 \nL 219.224043 97.548669 \nL 221.273975 97.731539 \nL 223.323908 97.609626 \nL 225.37384 97.630525 \nL 225.403125 97.62337 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 30.103125 124.315696 \nL 40.382072 120.048744 \nL 50.66102 120.361342 \nL 60.939967 116.375728 \nL 71.218914 112.687081 \nL 81.497862 110.795868 \nL 91.776809 111.139725 \nL 102.055757 109.654888 \nL 112.334704 108.592058 \nL 122.613651 108.638947 \nL 132.892599 107.685526 \nL 143.171546 107.732416 \nL 153.450493 107.388559 \nL 163.729441 107.904344 \nL 174.008388 107.24789 \nL 184.287336 107.482338 \nL 194.566283 107.122851 \nL 204.84523 106.700845 \nL 215.124178 106.919663 \nL 225.403125 107.23226 \n\" clip-path=\"url(#p512315e8e1)\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 148.385069 \nL 30.103125 9.785069 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 225.403125 148.385069 \nL 225.403125 9.785069 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 148.385069 \nL 225.403125 148.385069 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 9.785069 \nL 225.403125 9.785069 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 140.634375 61.819444 \nL 218.403125 61.819444 \nQ 220.403125 61.819444 220.403125 59.819444 \nL 220.403125 16.785069 \nQ 220.403125 14.785069 218.403125 14.785069 \nL 140.634375 14.785069 \nQ 138.634375 14.785069 138.634375 16.785069 \nL 138.634375 59.819444 \nQ 138.634375 61.819444 140.634375 61.819444 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 142.634375 22.883506 \nL 152.634375 22.883506 \nL 162.634375 22.883506 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- train loss -->\n     <g transform=\"translate(170.634375 26.383506) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"292.333984\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"353.515625\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"405.615234\"/>\n     </g>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 142.634375 37.561631 \nL 152.634375 37.561631 \nL 162.634375 37.561631 \n\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #bf00bf; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- train acc -->\n     <g transform=\"translate(170.634375 41.061631) scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"232.763672\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"264.550781\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"325.830078\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"380.810547\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 142.634375 52.239756 \nL 152.634375 52.239756 \nL 162.634375 52.239756 \n\" style=\"fill: none; stroke-dasharray: 9.6,2.4,1.5,2.4; stroke-dashoffset: 0; stroke: #008000; stroke-width: 1.5\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- valid acc -->\n     <g transform=\"translate(170.634375 55.739756) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"148.242188\"/>\n      <use xlink:href=\"#DejaVuSans-64\" x=\"176.025391\"/>\n      <use xlink:href=\"#DejaVuSans-20\" x=\"239.501953\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"271.289062\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"332.568359\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"387.548828\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p512315e8e1\">\n   <rect x=\"30.103125\" y=\"9.785069\" width=\"195.3\" height=\"138.6\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "devices, num_epochs, lr, wd = d2l.try_all_gpus(), 20, 2e-4, 5e-4\n",
        "lr_period, lr_decay, net = 4, 0.5, get_net()\n",
        "net(next(iter(train_iter))[0])\n",
        "start_time3 = time.time()\n",
        "train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\n",
        "      lr_decay)\n",
        "time_taken = time.time() - start_time3\n",
        "time_per_epoch = time_taken / num_epochs\n",
        "print(f\"{(time_taken):.2f} seconds to execute {num_epochs} epochs\")\n",
        "print(f\"Therefore {time_per_epoch:.2f} seconds per epoch\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}